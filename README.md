# Lakehouse Demo - Azure Data Engineering

## ğŸ“‹ Project Overview

This project demonstrates a **modern Data Lakehouse architecture** on Azure, 
built on the **WideWorldImporters (WWI)** sample dataset. Raw data flows through 
three medallion layers â€” **Bronze (raw)** â†’ **Silver (cleaned)** â†’ **Gold (aggregated)** â€” 
processed by Python-based ETL pipelines with automated testing and deployment via GitHub Actions.

---

## ğŸ—ï¸ Architecture Overview

```
Bronze Layer          Silver Layer          Gold Layer
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Raw CSV files    â†’    Cleaned Parquet   â†’    Business-ready
(Azure Blob)          (typed, validated)     Parquet / Power BI
```

**Data Foundation:** Microsoft WideWorldImporters (WWI) â€“ a realistic dataset 
for an international trading company covering sales, purchasing, and warehouse operations.

---

## ğŸ“ Project Structure

```
lakehouse-demo/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/          # Raw CSV files (source of truth)
â”‚   â”œâ”€â”€ silver/          # Cleaned Parquet files
â”‚   â”‚   â”œâ”€â”€ sales/
â”‚   â”‚   â”œâ”€â”€ purchasing/
â”‚   â”‚   â””â”€â”€ dimensions/
â”‚   â””â”€â”€ gold/            # Aggregated, analytics-ready data (coming soon)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ base_transformer.py       # Abstract base class for all transformers
â”‚   â”‚   â”œâ”€â”€ sales/                    # Sales transformers
â”‚   â”‚   â”œâ”€â”€ purchasing/               # Purchasing transformers
â”‚   â”‚   â””â”€â”€ dimensions/               # Dimension transformers
â”‚   â”œâ”€â”€ upload/                       # Azure Blob Storage upload
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ dimensions.yml            # Config-driven dimension transformer
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ tests/                            # pytest test suite (coming soon)
â”œâ”€â”€ run_all.py                        # Single entry point for full pipeline
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ environment.yml
â””â”€â”€ .github/
    â””â”€â”€ workflows/                    # GitHub Actions CI/CD (coming soon)
```

---

## ğŸ¥‰ Bronze Layer

- Raw CSV files from WWI dataset, uploaded as-is to **Azure Blob Storage**
- No transformations applied â€“ permanent source of truth
- 21 tables across Sales, Purchasing, and Application domains

---

## ğŸ¥ˆ Silver Layer

Cleaned and typed Parquet files, organized by domain:

| Domain | Tables |
|---|---|
| **Sales** | orders, order_lines, customers, invoices, invoice_lines |
| **Purchasing** | purchase_orders, purchase_order_lines, suppliers, supplier_transactions |
| **Dimensions** | cities, countries, provinces, people, delivery_methods, payment_methods, transaction_types, colors, package_types, stock_groups, stock_items, stock_item_holdings |

**Transformations applied:**
- PascalCase â†’ snake_case column renaming
- String â†’ `datetime64[ns]` casting for all date columns
- Nullable float IDs â†’ `Int64` (pandas nullable integer)
- Empty columns dropped (documented per table)
- Null values validated and documented with business reasoning

---

## ğŸ¥‡ Gold Layer *(in progress)*

Business-ready aggregations and Star Schema for analytics:

- `fact_orders` â€“ order lines joined with orders and customers
- `fact_invoices` â€“ invoice lines with revenue metrics
- `dim_customers` â€“ enriched customer dimension
- `dim_products` â€“ stock items with categories and suppliers
- Power BI reports on top of Gold layer

---

## âš™ï¸ ETL Design

The ETL is built on an **OOP inheritance pattern**:

```
BaseTransformer (abstract)
â”œâ”€â”€ load_bronze()         # shared
â”œâ”€â”€ _drop_empty_columns() # shared
â”œâ”€â”€ _to_datetime()        # shared helper
â”œâ”€â”€ save_silver()         # shared
â”œâ”€â”€ run()                 # shared orchestration
â””â”€â”€ transform()           # abstract â†’ implemented per table

    â”œâ”€â”€ OrderTransformer
    â”œâ”€â”€ CustomerTransformer
    â”œâ”€â”€ DimensionTransformer  â† config-driven (dimensions.yml)
    â””â”€â”€ ...
```

Run the full pipeline with a single command:
```bash
python run_all.py
```

---

## â˜ï¸ Azure Integration

- **Azure Blob Storage** â€“ Bronze and Silver layers stored in containers
- **Azure Data Factory** â€“ Pipeline orchestration *(coming soon)*
- **GitHub Actions** â€“ CI/CD for automated testing and deployment *(coming soon)*

---

## ğŸ› ï¸ Setup

**1. Clone the repository**
```bash
git clone https://github.com/mibeit/lakehouse-demo.git
cd lakehouse-demo
```

**2. Create Conda environment**
```bash
conda env create -f environment.yml
conda activate lakehouse-demo
```

**3. Install as editable package**
```bash
pip install -e .
```

**4. Configure environment variables**
```bash
cp .env.example .env
# Add your Azure Storage connection string
```

**5. Run the full pipeline**
```bash
python run_all.py
```

---

## ğŸ“Š Dataset

**WideWorldImporters** is Microsoft's sample database for an international 
wholesale novelty goods importer. It covers:
- 40,000+ sales orders
- 127,000+ order lines
- 625 customers across multiple territories
- 227 stock items across 10 product groups
- Full purchasing and supplier transaction history

---

## ğŸ—ºï¸ Roadmap

- [x] Bronze Layer â€“ raw CSV upload to Azure Blob Storage
- [x] Silver Layer â€“ 21 tables cleaned and validated
- [ ] Gold Layer â€“ Star Schema + business aggregations
- [ ] Azure Data Factory pipeline orchestration
- [ ] GitHub Actions CI/CD
- [ ] Power BI reports
- [ ] Monitoring and alerting
